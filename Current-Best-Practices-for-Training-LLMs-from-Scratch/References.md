## References

- [What Language Model Architecture and pre-training Objective Work Best for Zero-Shot Generalization?](https://arxiv.org/abs/2102.07350)  
- [Language Models are Few-Shot Learners (GPT-3 Paper)](https://arxiv.org/abs/2005.14165)  
- [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)  
- [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)  
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)  
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473)  
- [Scalable Deep Learning on Distributed Infrastructures: Challenges, Techniques and Tools](https://arxiv.org/abs/2202.08309)  
- [New Scaling Laws for Large Language Models (DeepMind)](https://arxiv.org/abs/2203.15556)  
- [Understanding the Difficulty of Training Transformers](https://arxiv.org/abs/2004.08249)  
- [How To Build an Efficient NLP Model](https://arxiv.org/abs/2003.11254)  
- [Emergent Abilities of Large Language Models](https://arxiv.org/abs/2206.07682)  
- [Beyond the Imitation Game Benchmark (BIG-bench)](https://arxiv.org/abs/2206.04615)  
- [Talking About Large Language Models](https://arxiv.org/abs/2212.03551)  
- [Galactica: A Large Language Model for Science](https://arxiv.org/abs/2211.09085)  
- [State of AI Report 2022](https://www.stateof.ai/)  
- [Finetuned Language Models are Zero-Shot Learners](https://arxiv.org/abs/2109.01652)  
- [Scaling Instruction-Fine Tuned Language Models](https://arxiv.org/abs/2210.11416)  
- [Training Language Models to Follow Instructions with Human Feedback (InstructGPT / RLHF)](https://arxiv.org/abs/2203.02155)  
